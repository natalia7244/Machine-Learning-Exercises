{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1RtXXaoW7yaNE8N-_B7biPyolTiZtWobE",
      "authorship_tag": "ABX9TyNF+H/vUZT6HAjGZbsCIpea",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/natalia7244/Machine-Learning-Exercises/blob/main/Cross_Validation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction"
      ],
      "metadata": {
        "id": "pj-_rOmWAFxX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Machine learning is something we do step by step. At each step, we must choose things like:\n",
        "\n",
        "    What data (columns) to use\n",
        "\n",
        "    What type of model to try\n",
        "\n",
        "    What settings (parameters) to give the model\n",
        "\n",
        "To see which model is better, we test them. We don't test on the same data the model learned from. Instead, we save part of the data to test — this is called a validation set.\n",
        "\n",
        "For example, if we have 5000 rows of data, we might use 1000 rows to test and 4000 to train. This helps us see how well the model works on new data.\n",
        "\n",
        "But there's a problem: the results can change depending on which 1000 rows we choose for testing. One model might look good just by luck — not because it’s really better.\n",
        "\n",
        "If we use only 1 row for testing, it’s too small. That’s almost just guessing!\n",
        "\n",
        "If we use a big test set, we get better (less random) results. But then we have less data to train, and that can make the model worse.\n",
        "\n",
        "So we must find a balance: enough test data to trust the result, but enough training data to build a good model."
      ],
      "metadata": {
        "id": "_dd9Xw5BALNL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is cross-validation?"
      ],
      "metadata": {
        "id": "b0bu0VBABiQE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cross-validation is a smart way to test how good a machine learning model is.\n",
        "\n",
        "We do this by splitting our data into small parts — for example, 5 equal parts. These parts are called folds.\n",
        "\n",
        "Then we do 5 tests (experiments):\n",
        "\n",
        "    In the first test, we use the first part to check the model (validation), and the other 4 parts to train the model.\n",
        "\n",
        "    In the second test, we use the second part to check the model, and the rest to train.\n",
        "\n",
        "    We repeat this, each time changing which part we use for checking.\n",
        "\n",
        "In the end:\n",
        "\n",
        "    Every row in the data is used once to test the model.\n",
        "\n",
        "    We get 5 different scores (one from each fold).\n",
        "\n",
        "    We can take the average of those scores to understand how good the model is.\n",
        "\n",
        "This way, the result is more fair and reliable, because we used all the data — just not all at once."
      ],
      "metadata": {
        "id": "FMfAba0nFFTM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# When should you use cross-validation?"
      ],
      "metadata": {
        "id": "MGe_65j_FHvV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cross-validation gives a better (more correct) way to check how good your model is. This is helpful when you need to make many decisions about the model.\n",
        "\n",
        "But: cross-validation is slower because it trains the model many times — one time for each fold.\n",
        "\n",
        "So, when should you use cross-validation, and when just one validation set?\n",
        "\n",
        "    If you have a small dataset, and your code is not very slow, then use cross-validation. It gives better results.\n",
        "\n",
        "    If you have a big dataset, using just one validation set is usually enough. It runs faster, and you still get good results.\n",
        "\n",
        "There is no exact rule for what is small or big data. But:\n",
        "\n",
        "    If your model runs in a few minutes or less, then it’s probably OK to use cross-validation.\n",
        "\n",
        "    You can also try cross-validation once. If all the test results (folds) are similar, then using one validation set later is probably fine."
      ],
      "metadata": {
        "id": "9M6yr8njFOpC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example"
      ],
      "metadata": {
        "id": "gnsqNcwcH8iY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv('/content/drive/MyDrive/Data_sets/melb_data.csv')\n",
        "\n",
        "cols_to_use = ['Rooms', 'Distance', 'Landsize', 'BuildingArea', 'YearBuilt']\n",
        "X = data[cols_to_use]\n",
        "\n",
        "y = data.Price\n",
        "\n",
        "\n",
        "# Using a pipeline will make the code remarkable straightforward\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "my_pipeline = Pipeline( steps = [('preprocessor', SimpleImputer()),\n",
        "                                 ('model', RandomForestRegressor(n_estimators =50, random_state =0))\n",
        "                                 ])"
      ],
      "metadata": {
        "id": "iUl2vJbEH-vq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cross_val_score"
      ],
      "metadata": {
        "id": "ULfVSGXrP5rp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "scores = -1 * cross_val_score(my_pipeline, X, y, cv=5, scoring = 'neg_mean_absolute_error')\n",
        "\n",
        "print(\"MAE scores: \\n\", scores)\n",
        "\n",
        "print(\"Average MAE score:\")\n",
        "print(scores.mean())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IH6O9UJCP-Q0",
        "outputId": "9ca6c183-8dc7-44eb-82e3-8f75bd9437e4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE scores: \n",
            " [301628.7893587  303164.4782723  287298.331666   236061.84754543\n",
            " 260383.45111427]\n",
            "Average MAE score:\n",
            "277707.3795913405\n"
          ]
        }
      ]
    }
  ]
}